#!/usr/bin/env node
/**
 * Multi-Model MCP Server v4.0
 *
 * ì—°ê²° ëª¨ë¸:
 *   - GPT-5.3-Codex  : ChatGPT OAuth (~/.codex/auth.json) â†’ Responses API /v1/responses
 *   - Gemini 2.5 Pro : AI Studio API Key â†’ OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
 *   - GLM-4.7-Flash  : Z.ai API Key     â†’ OpenAI í˜¸í™˜ ì—”ë“œí¬ì¸íŠ¸
 *
 * v4.0 ì‹ ê·œ:
 *   - fetchWithRetry: 429/500/502/503/529 â†’ ìµœëŒ€ 3íšŒ ì¬ì‹œë„, ì§€ìˆ˜ ë°±ì˜¤í”„
 *   - smart_route   : ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ìë™ ë¼ìš°íŒ… + í´ë°± ì²´ì¸
 *   - ask_parallel  : Promise.allSettled() ë‹¤ì¤‘ ëª¨ë¸ ë™ì‹œ í˜¸ì¶œ
 *   - í™•ì¥ íŒŒë¼ë¯¸í„° : max_tokens, temperature (Gemini/GLM), max_tokens (GPT)
 *   - ê°•í™” ë¡œê¹…     : category, retry_count, routing í•„ë“œ ì¶”ê°€
 */

import { Server } from "@modelcontextprotocol/sdk/server/index.js";
import { StdioServerTransport } from "@modelcontextprotocol/sdk/server/stdio.js";
import {
  CallToolRequestSchema,
  ListToolsRequestSchema,
} from "@modelcontextprotocol/sdk/types.js";
import { readFileSync, writeFileSync, existsSync, appendFileSync, mkdirSync } from "fs";
import { homedir } from "os";
import { join, dirname } from "path";

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// í† í° ì‚¬ìš©ëŸ‰ ë¡œê¹…
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const USAGE_LOG_PATH = join(homedir(), "mcp-servers", "multi-model", "usage-log.jsonl");
const LAST_ROUTE_PATH = join(homedir(), "mcp-servers", "multi-model", "last-route.json");

// ì¹´í…Œê³ ë¦¬ë³„ í•œêµ­ì–´ ì´ìœ  ì„¤ëª…
const CATEGORY_REASON = {
  ultrabrain: "ì „ì²´ ì•„í‚¤í…ì²˜ ì„¤ê³„ Â· ì¢…í•© ì „ëµ ê²°ì •",
  deep:       "ì•Œê³ ë¦¬ì¦˜ ë¶„ì„ Â· ë³µì¡í•œ ë””ë²„ê¹… Â· ë¦¬íŒ©í† ë§",
  visual:     "UI/UX Â· React/Vue Â· í”„ë¡ íŠ¸ì—”ë“œ ì‘ì—…",
  research:   "ì½”ë“œë² ì´ìŠ¤ ì „ì²´ ë¶„ì„ Â· ëŒ€ìš©ëŸ‰ íŒŒì¼",
  bulk:       "ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ Â· CRUD Â· ë°˜ë³µ íŒ¨í„´ ìƒì„±",
  writing:    "ë¬¸ì„œ ì‘ì„± Â· README Â· ì£¼ì„ ì¶”ê°€",
  quick:      "ë‹¨ìˆœ ë³€í™˜ Â· í¬ë§·íŒ… Â· ì¦‰ê° ì²˜ë¦¬",
};

// ëª¨ë¸ í‘œì‹œëª…
const MODEL_DISPLAY = {
  gpt:    "GPT-5.3-Codex",
  gemini: "Gemini 2.5 Pro",
  glm:    "GLM-4.7-Flash",
};

function logUsage(model, inputTokens, outputTokens, extra = {}) {
  const entry = {
    timestamp: new Date().toISOString(),
    model,
    input_tokens: inputTokens,
    output_tokens: outputTokens,
    total_tokens: inputTokens + outputTokens,
    ...extra, // category, retry_count, routing, reasoning_effort ë“±
  };
  try {
    mkdirSync(dirname(USAGE_LOG_PATH), { recursive: true });
    appendFileSync(USAGE_LOG_PATH, JSON.stringify(entry) + "\n");
  } catch {
    // ë¡œê¹… ì‹¤íŒ¨í•´ë„ ë©”ì¸ ê¸°ëŠ¥ì— ì˜í–¥ ì—†ìŒ
  }
}

// ë¼ìš°íŒ… íŠ¸ë ˆì´ìŠ¤ë¥¼ last-route.jsonì— ì €ì¥ (routing-display.js í›…ì´ ì½ìŒ)
function saveRoutingTrace(trace) {
  try {
    mkdirSync(dirname(LAST_ROUTE_PATH), { recursive: true });
    writeFileSync(LAST_ROUTE_PATH, JSON.stringify({ ...trace, timestamp: new Date().toISOString() }));
  } catch {
    // ì €ì¥ ì‹¤íŒ¨í•´ë„ ë¬´ì‹œ
  }
}

// ì‘ë‹µì— í¬í•¨ë  ë¼ìš°íŒ… í—¤ë” (ìœ ë‹ˆì½”ë“œ ë°•ìŠ¤ ì•„íŠ¸, ANSI ì—†ìŒ)
function formatRoutingHeader({ cat, model, effort = null, didFallback = false, fallbackFrom = null }) {
  const modelName = MODEL_DISPLAY[model] ?? model;
  const effortStr = effort && effort !== "none" ? ` Â· reasoning: ${effort}` : "";
  const reason = CATEGORY_REASON[cat] ?? cat;
  const lines = [
    `â•­â”€ ğŸ”€ ROUTING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`,
    `â”‚  ì¹´í…Œê³ ë¦¬ : ${cat}`,
    `â”‚  ëª¨    ë¸ : ${modelName}${effortStr}`,
    `â”‚  ì´    ìœ  : ${reason}`,
  ];
  if (didFallback && fallbackFrom) {
    lines.push(`â”‚  âš  í´ë°±  : ${MODEL_DISPLAY[fallbackFrom] ?? fallbackFrom} ì‹¤íŒ¨ â†’ ${modelName}`);
  }
  lines.push(`â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€`);
  return lines.join("\n");
}

function getUsageStats(days = 7) {
  if (!existsSync(USAGE_LOG_PATH)) return "ì‚¬ìš© ê¸°ë¡ ì—†ìŒ (ì•„ì§ ì™¸ë¶€ ëª¨ë¸ í˜¸ì¶œ ì—†ìŒ)";

  const lines = readFileSync(USAGE_LOG_PATH, "utf8")
    .trim()
    .split("\n")
    .filter(Boolean);

  const cutoff = new Date();
  cutoff.setDate(cutoff.getDate() - days);

  const byModel = {};
  const byCategory = {};
  let grandInput = 0, grandOutput = 0, grandCalls = 0;

  for (const line of lines) {
    try {
      const e = JSON.parse(line);
      if (new Date(e.timestamp) < cutoff) continue;

      // ëª¨ë¸ë³„ ì§‘ê³„
      if (!byModel[e.model]) byModel[e.model] = { input: 0, output: 0, calls: 0 };
      byModel[e.model].input += e.input_tokens;
      byModel[e.model].output += e.output_tokens;
      byModel[e.model].calls++;
      grandInput += e.input_tokens;
      grandOutput += e.output_tokens;
      grandCalls++;

      // ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
      if (e.category) {
        if (!byCategory[e.category]) byCategory[e.category] = 0;
        byCategory[e.category]++;
      }
    } catch { /* skip malformed */ }
  }

  if (grandCalls === 0) return `ìµœê·¼ ${days}ì¼ê°„ ì™¸ë¶€ ëª¨ë¸ í˜¸ì¶œ ì—†ìŒ`;

  const grandTotal = grandInput + grandOutput;

  const out = [
    `=== ì™¸ë¶€ ëª¨ë¸ í† í° ì‚¬ìš© í˜„í™© (ìµœê·¼ ${days}ì¼) ===`,
    "",
    `${"ëª¨ë¸".padEnd(18)} ${"í˜¸ì¶œ".padStart(5)} ${"ì…ë ¥".padStart(9)} ${"ì¶œë ¥".padStart(8)} ${"í•©ê³„".padStart(9)} ${"ë¹„ìœ¨".padStart(6)}`,
    "â”€".repeat(60),
  ];

  for (const [model, s] of Object.entries(byModel)) {
    const total = s.input + s.output;
    const pct = ((total / grandTotal) * 100).toFixed(1);
    out.push(
      `${model.padEnd(18)} ${String(s.calls).padStart(5)} ${String(s.input).padStart(9)} ${String(s.output).padStart(8)} ${String(total).padStart(9)} ${(pct + "%").padStart(6)}`
    );
  }

  out.push("â”€".repeat(60));
  out.push(
    `${"í•©ê³„".padEnd(18)} ${String(grandCalls).padStart(5)} ${String(grandInput).padStart(9)} ${String(grandOutput).padStart(8)} ${String(grandTotal).padStart(9)} ${"100%".padStart(6)}`
  );

  if (Object.keys(byCategory).length > 0) {
    out.push("");
    out.push("=== ì¹´í…Œê³ ë¦¬ë³„ í˜¸ì¶œ ===");
    for (const [cat, cnt] of Object.entries(byCategory)) {
      out.push(`  ${cat.padEnd(12)}: ${cnt}íšŒ`);
    }
  }

  out.push("");
  out.push("â€» Claude í† í°ì€ claude.ai/settings/billing ì—ì„œ í™•ì¸");

  return out.join("\n");
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// fetchWithRetry â€” ì¬ì‹œë„ ë¡œì§
// 429/500/502/503/529 â†’ ìµœëŒ€ 3íšŒ, ì§€ìˆ˜ ë°±ì˜¤í”„ + ëœë¤ ì§€í„°
// 400/401/404 â†’ ì¦‰ì‹œ ì‹¤íŒ¨
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const RETRYABLE_STATUS = new Set([429, 500, 502, 503, 529]);

async function fetchWithRetry(url, options, maxRetries = 3) {
  let retryCount = 0;
  while (true) {
    const res = await fetch(url, options);
    if (res.ok) return { res, retryCount };
    if (!RETRYABLE_STATUS.has(res.status) || retryCount >= maxRetries) {
      return { res, retryCount };
    }
    const delay = Math.pow(2, retryCount) * 1000 + Math.random() * 500;
    await new Promise((r) => setTimeout(r, delay));
    retryCount++;
  }
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// GPT OAuth í† í° ê´€ë¦¬
// auth.json êµ¬ì¡°: { tokens: { access_token, refresh_token }, OPENAI_API_KEY, last_refresh }
// ìš°ì„ ìˆœìœ„: 1) OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜  2) auth.jsonì˜ OPENAI_API_KEY  3) ChatGPT OAuth
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const CODEX_AUTH_PATH = join(homedir(), ".codex", "auth.json");
const TOKEN_REFRESH_URL = "https://auth.openai.com/oauth/token";

let refreshPromise = null;

function readAuthJson() {
  if (!existsSync(CODEX_AUTH_PATH)) return null;
  try {
    return JSON.parse(readFileSync(CODEX_AUTH_PATH, "utf8"));
  } catch {
    return null;
  }
}

function getJwtInfo(token) {
  try {
    const payload = token.split(".")[1];
    const padded = payload + "=".repeat((4 - (payload.length % 4)) % 4);
    const decoded = JSON.parse(Buffer.from(padded, "base64").toString("utf8"));
    return {
      exp: decoded.exp ?? 0,
      scopes: decoded.scp ?? decoded.scope ?? [],
      clientId: decoded.client_id ?? null,
    };
  } catch {
    return { exp: 0, scopes: [], clientId: null };
  }
}

// ChatGPT JWTì—ì„œ https://api.openai.com/auth í´ë ˆì„ ì¶”ì¶œ (account_id ë“±)
function getJwtAuthClaim(token) {
  try {
    const payload = token.split(".")[1];
    const padded = payload + "=".repeat((4 - (payload.length % 4)) % 4);
    const decoded = JSON.parse(Buffer.from(padded, "base64").toString("utf8"));
    return decoded["https://api.openai.com/auth"] ?? null;
  } catch {
    return null;
  }
}

const SCOPE_RE_LOGIN_MSG =
  "Windowsì—ì„œ `codex login` ì‹¤í–‰ í›„ auth.jsonì„ ì´ ì„œë²„ë¡œ ë³µì‚¬í•˜ì„¸ìš”:\n" +
  "  scp ~/.codex/auth.json root@<ì„œë²„IP>:~/.codex/auth.json\n" +
  "â€» OpenAI refresh grantê°€ api.responses.write ìŠ¤ì½”í”„ë¥¼ ìœ ì§€í•˜ì§€ ì•ŠëŠ” ì œí•œì…ë‹ˆë‹¤.";

async function doRefreshToken(refreshToken, clientId) {
  const body = { grant_type: "refresh_token", refresh_token: refreshToken };
  if (clientId) body.client_id = clientId;
  const res = await fetch(TOKEN_REFRESH_URL, {
    method: "POST",
    headers: { "Content-Type": "application/json" },
    body: JSON.stringify(body),
  });
  if (!res.ok) {
    const err = await res.text();
    throw new Error(`í† í° ê°±ì‹  ì‹¤íŒ¨ (HTTP ${res.status}): ${err}`);
  }
  return await res.json();
}

async function getValidAccessToken() {
  // â”€â”€ 1ìˆœìœ„: OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  if (process.env.OPENAI_API_KEY) {
    return { token: process.env.OPENAI_API_KEY, isOAuthOnly: false };
  }

  // â”€â”€ 2ìˆœìœ„: auth.jsonì˜ OPENAI_API_KEY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  const auth = readAuthJson();
  if (auth?.OPENAI_API_KEY) {
    return { token: auth.OPENAI_API_KEY, isOAuthOnly: false };
  }

  // â”€â”€ 3ìˆœìœ„: ChatGPT OAuth access_token â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  if (!auth) {
    throw new Error(
      "GPT ì¸ì¦ ì—†ìŒ. ë‹¤ìŒ ì¤‘ í•˜ë‚˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”:\n" +
      "  ë°©ë²• 1) OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ ì„¤ì • (platform.openai.comì—ì„œ ë°œê¸‰)\n" +
      "  ë°©ë²• 2) ~/.codex/auth.jsonì˜ OPENAI_API_KEY í•„ë“œì— sk-... í‚¤ ì…ë ¥\n" +
      "  ë°©ë²• 3) codex login ìœ¼ë¡œ ChatGPT OAuth ì¸ì¦ (api.responses.write ìŠ¤ì½”í”„ í•„ìš”)"
    );
  }

  const tokens = auth.tokens ?? auth;
  const accessToken = tokens.access_token;
  const refreshToken = tokens.refresh_token;

  if (!accessToken) {
    throw new Error("access_tokenì´ ì—†ìŠµë‹ˆë‹¤. `codex login`ìœ¼ë¡œ ì¬ì¸ì¦ì´ í•„ìš”í•©ë‹ˆë‹¤.");
  }

  const { exp: expiry, scopes, clientId } = getJwtInfo(accessToken);
  // OAuth ì „ìš©(api.responses.write ìŠ¤ì½”í”„ ì—†ìŒ) ì‹œ Chat Completions API í´ë°± ì‚¬ìš©
  const isOAuthOnly = !scopes.includes("api.responses.write");
  const isExpired = expiry > 0 && Date.now() / 1000 > expiry - 60;
  if (!isExpired) return { token: accessToken, isOAuthOnly };
  if (!refreshToken) {
    throw new Error(
      "í† í°ì´ ë§Œë£Œë˜ì—ˆê³  refresh_tokenì´ ì—†ìŠµë‹ˆë‹¤. `codex login`ìœ¼ë¡œ ì¬ì¸ì¦í•˜ì„¸ìš”."
    );
  }
  if (!refreshPromise) {
    refreshPromise = doRefreshToken(refreshToken, clientId)
      .then((refreshed) => {
        const { scopes: newScopes } = getJwtInfo(refreshed.access_token);
        const newAuth = {
          ...auth,
          tokens: {
            ...(auth.tokens ?? {}),
            access_token: refreshed.access_token,
            ...(refreshed.refresh_token && { refresh_token: refreshed.refresh_token }),
          },
          last_refresh: new Date().toISOString(),
        };
        writeFileSync(CODEX_AUTH_PATH, JSON.stringify(newAuth, null, 2));
        const refreshedIsOAuthOnly = !newScopes.includes("api.responses.write");
        return { token: refreshed.access_token, isOAuthOnly: refreshedIsOAuthOnly };
      })
      .finally(() => { refreshPromise = null; });
  }

  try {
    return await refreshPromise;
  } catch (e) {
    throw new Error(`ì•¡ì„¸ìŠ¤ í† í° ê°±ì‹  ì‹¤íŒ¨: ${e.message}`);
  }
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// callGpt â€” Responses API
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function callGpt(
  prompt,
  model = "gpt-5.3-codex",
  systemPrompt = null,
  reasoningEffort = "medium",
  maxTokens = null,
  logExtra = {}
) {
  const { token, isOAuthOnly } = await getValidAccessToken();

  // OAuth ì „ìš© í† í°: chatgpt.com/backend-api/codex/responses ì‚¬ìš©
  // (api.responses.write ìŠ¤ì½”í”„ ì—†ëŠ” ChatGPT Plus/Pro OAuth í† í°ìš©)
  if (isOAuthOnly) {
    const authClaim = getJwtAuthClaim(token);
    const accountId = authClaim?.chatgpt_account_id;
    if (!accountId) {
      throw new Error(
        "ChatGPT account_idë¥¼ JWTì—ì„œ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. `codex login`ìœ¼ë¡œ ì¬ì¸ì¦í•˜ì„¸ìš”."
      );
    }

    const instructions = systemPrompt || "You are a helpful coding assistant.";
    const codexBody = {
      model, instructions, store: false, stream: true,
      input: [{ role: "user", content: prompt }],
    };
    if (reasoningEffort !== "none") codexBody.reasoning = { effort: reasoningEffort };
    if (maxTokens) codexBody.max_output_tokens = maxTokens;

    const { res: codexRes, retryCount: codexRetry } = await fetchWithRetry(
      "https://chatgpt.com/backend-api/codex/responses",
      {
        method: "POST",
        headers: {
          Authorization: `Bearer ${token}`,
          "Content-Type": "application/json",
          "chatgpt-account-id": accountId,
          "OpenAI-Beta": "responses=experimental",
          originator: "codex_cli_rs",
        },
        body: JSON.stringify(codexBody),
      }
    );

    if (!codexRes.ok) {
      const errText = await codexRes.text();
      throw new Error(
        `GPT Codex backend ì˜¤ë¥˜ (HTTP ${codexRes.status}): ${errText.substring(0, 300)}\n` +
        `â€» ì´ ì˜¤ë¥˜ê°€ ê³„ì†ë˜ë©´ OPENAI_API_KEYë¥¼ platform.openai.comì—ì„œ ë°œê¸‰í•˜ì„¸ìš”.`
      );
    }

    // SSE â†’ í…ìŠ¤íŠ¸ íŒŒì‹±
    const sseText = await codexRes.text();
    const sseLines = sseText.split("\n");
    let completedResponse = null;
    for (let i = 0; i < sseLines.length; i++) {
      if (sseLines[i].startsWith("event: response.completed")) {
        const dataLine = sseLines[i + 1];
        if (dataLine?.startsWith("data: ")) {
          try { completedResponse = JSON.parse(dataLine.slice(6)); } catch { /* skip */ }
        }
      }
    }

    if (completedResponse) {
      const usage = completedResponse.response?.usage ?? {};
      logUsage(model, usage.input_tokens ?? 0, usage.output_tokens ?? 0, {
        retry_count: codexRetry, routing: "chatgpt_codex_backend", ...logExtra,
      });
      const texts = [];
      for (const item of completedResponse.response?.output ?? []) {
        if (item.type === "message") {
          for (const block of item.content ?? []) {
            if (block.type === "output_text" && block.text) texts.push(block.text);
          }
        }
      }
      if (texts.length > 0) return texts.join("\n");
    }

    // fallback: delta ì¡°í•©
    const deltas = [];
    for (let i = 0; i < sseLines.length; i++) {
      if (sseLines[i].startsWith("event: response.output_text.delta")) {
        const dataLine = sseLines[i + 1];
        if (dataLine?.startsWith("data: ")) {
          try { const d = JSON.parse(dataLine.slice(6)); if (d.delta) deltas.push(d.delta); } catch { /* skip */ }
        }
      }
    }
    if (deltas.length > 0) return deltas.join("");
    return "[GPT Codex backend: ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨]";
  }

  const input = [];
  // Responses API (ì •ìƒ ê²½ë¡œ)
  if (systemPrompt) input.push({ role: "system", content: systemPrompt });
  input.push({ role: "user", content: prompt });
  const body = { model, input };
  if (reasoningEffort !== "none") body.reasoning = { effort: reasoningEffort };
  if (maxTokens) body.max_output_tokens = maxTokens;
  const { res, retryCount } = await fetchWithRetry("https://api.openai.com/v1/responses", {
    method: "POST",
    headers: {
      Authorization: `Bearer ${token}`,
      "Content-Type": "application/json",
    },
    body: JSON.stringify(body),
  });
  if (!res.ok) {
    const err = await res.text();
    throw new Error(`GPT API ì˜¤ë¥˜ (HTTP ${res.status}): ${err}`);
  }
  const data = await res.json();
  const usage = data.usage ?? {};
  logUsage(model, usage.input_tokens ?? 0, usage.output_tokens ?? 0, {
    reasoning_tokens: usage.reasoning_tokens ?? 0,
    reasoning_effort: reasoningEffort,
    retry_count: retryCount,
    ...logExtra,
  });
  const texts = [];
  for (const item of data.output ?? []) {
    if (item.type === "message") {
      for (const block of item.content ?? []) {
        if (block.type === "output_text" && block.text) texts.push(block.text);
      }
    }
  }
  if (texts.length === 0) {
    return `[ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨] raw: ${JSON.stringify(data.output ?? data).slice(0, 500)}`;
  }
  return texts.join("\n");
}
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// callGemini â€” OpenAI í˜¸í™˜ Chat Completions
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function callGemini(
  prompt,
  model = "gemini-2.5-pro",
  systemPrompt = null,
  maxTokens = null,
  temperature = null,
  logExtra = {}
) {
  const apiKey = process.env.GEMINI_API_KEY;
  if (!apiKey) {
    throw new Error(
      "GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ ì—†ìŒ. ~/.bashrcì— export GEMINI_API_KEY='í‚¤ê°’' ì¶”ê°€ í›„ í„°ë¯¸ë„ ì¬ì‹œì‘."
    );
  }

  const messages = [];
  if (systemPrompt) messages.push({ role: "system", content: systemPrompt });
  messages.push({ role: "user", content: prompt });

  const body = { model, messages, max_tokens: maxTokens ?? 8192 };
  if (temperature !== null) body.temperature = temperature;

  const { res, retryCount } = await fetchWithRetry(
    "https://generativelanguage.googleapis.com/v1beta/openai/chat/completions",
    {
      method: "POST",
      headers: {
        Authorization: `Bearer ${apiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(body),
    }
  );

  if (!res.ok) {
    const err = await res.text();
    throw new Error(`Gemini API ì˜¤ë¥˜ (HTTP ${res.status}): ${err}`);
  }

  const data = await res.json();
  const usage = data.usage ?? {};
  logUsage(model, usage.prompt_tokens ?? 0, usage.completion_tokens ?? 0, {
    retry_count: retryCount,
    ...logExtra,
  });

  return data.choices?.[0]?.message?.content ?? "ì‘ë‹µ ì—†ìŒ";
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// callGlm â€” OpenAI í˜¸í™˜ Chat Completions (Z.ai)
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function callGlm(
  prompt,
  model = "glm-4.7-flash",
  systemPrompt = null,
  maxTokens = null,
  temperature = null,
  logExtra = {}
) {
  const apiKey = process.env.GLM_API_KEY;
  if (!apiKey) {
    throw new Error(
      "GLM_API_KEY í™˜ê²½ë³€ìˆ˜ ì—†ìŒ. api.z.aiì—ì„œ í‚¤ ë°œê¸‰ í›„ ~/.bashrcì— ì¶”ê°€."
    );
  }

  const messages = [];
  if (systemPrompt) messages.push({ role: "system", content: systemPrompt });
  messages.push({ role: "user", content: prompt });

  const body = { model, messages, max_tokens: maxTokens ?? 4096 };
  if (temperature !== null) body.temperature = temperature;

  const { res, retryCount } = await fetchWithRetry(
    "https://api.z.ai/api/paas/v4/chat/completions",
    {
      method: "POST",
      headers: {
        Authorization: `Bearer ${apiKey}`,
        "Content-Type": "application/json",
      },
      body: JSON.stringify(body),
    }
  );

  if (!res.ok) {
    const err = await res.text();
    throw new Error(`GLM API ì˜¤ë¥˜ (HTTP ${res.status}): ${err}`);
  }

  const data = await res.json();
  const usage = data.usage ?? {};
  logUsage(model, usage.prompt_tokens ?? 0, usage.completion_tokens ?? 0, {
    retry_count: retryCount,
    ...logExtra,
  });

  return data.choices?.[0]?.message?.content ?? "ì‘ë‹µ ì—†ìŒ";
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// smart_route â€” ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ ìë™ ë¼ìš°íŒ…
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const CATEGORY_PATTERNS = {
  ultrabrain: /ì•„í‚¤í…ì²˜.*ì„¤ê³„|ì„¤ê³„.*ì•„í‚¤í…ì²˜|system.?design|ì „ì²´.*ì „ëµ|ì „ëµ.*ê²°ì •|architecture|design.?pattern|comprehensive.*plan|ì „ì²´.*êµ¬ì¡°.*íŒŒì•…/i,
  deep: /ì•Œê³ ë¦¬ì¦˜|algorithm|ë³µì¡ë„|complexity|ìµœì í™”.*ë°©ë²•|optim.*approach|ë¦¬íŒ©í† ë§|refactor|ë²„ê·¸.*ë¶„ì„|deep.*debug|ì‹¬ì¸µ.*ë¶„ì„/i,
  visual: /\bUI\b|\bUX\b|ì»´í¬ë„ŒíŠ¸|component|\breact\b|\bvue\b|\bangular\b|tailwind|í™”ë©´.*ì„¤ê³„|ì„¤ê³„.*í™”ë©´|ìŠ¤í¬ë¦°ìƒ·|screenshot|í”„ë¡ íŠ¸ì—”ë“œ|frontend|CSS.*ë ˆì´ì•„ì›ƒ/i,
  research: /ì½”ë“œë² ì´ìŠ¤|codebase|ë ˆí¬.*ì „ì²´|ì „ì²´.*ë ˆí¬|entire.*repo|full.*codebase|íŒŒì¼.*ì—¬ëŸ¬.*ë¶„ì„|ì—¬ëŸ¬.*íŒŒì¼.*ë™ì‹œ|ì „ì²´.*íŒŒì¼.*êµ¬ì¡°/i,
  bulk: /ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸|boilerplate|\bCRUD\b|ë§ˆì´ê·¸ë ˆì´ì…˜|migration|ì‹œë“œ.*ë°ì´í„°|seed.*data|ë°˜ë³µ.*íŒ¨í„´.*ìƒì„±|ì—¬ëŸ¬.*ê°œ.*íŒŒì¼.*ìƒì„±|multiple.*similar.*files/i,
  writing: /ë¬¸ì„œ.*ì‘ì„±|ì‘ì„±.*ë¬¸ì„œ|\bREADME\b|ì£¼ì„.*ì¶”ê°€|ì¶”ê°€.*ì£¼ì„|ê¸°ìˆ .*ë¬¸ì„œ|technical.*doc|\bAPI.*docs?\b|javadoc|jsdoc/i,
  quick: /ê°„ë‹¨.*ë³€í™˜|ë³€í™˜.*ê°„ë‹¨|ë¹ ë¥´ê²Œ|í¬ë§·.*ë³€í™˜|format.*convert|ë‹¨ìˆœ.*ë³€ê²½|ë‹¨ìˆœ.*ìˆ˜ì •/i,
};

// ì¹´í…Œê³ ë¦¬ â†’ ë¼ìš°íŒ… ì„¤ì •
// fallbackEffort: í´ë°± ëª¨ë¸ì´ GPTì¼ ë•Œ ì‚¬ìš©í•  reasoning_effort
const CATEGORY_ROUTING = {
  ultrabrain: { model: "gpt",    effort: "xhigh", fallback: ["gemini"], fallbackEffort: "high"   },
  deep:       { model: "gpt",    effort: "high",  fallback: ["gemini"], fallbackEffort: null      },
  visual:     { model: "gemini", effort: null,     fallback: ["gpt"],    fallbackEffort: "high"   },
  research:   { model: "gemini", effort: null,     fallback: ["gpt"],    fallbackEffort: "high"   },
  bulk:       { model: "glm",    effort: null,     fallback: ["gemini"], fallbackEffort: null      },
  writing:    { model: "glm",    effort: null,     fallback: ["gemini"], fallbackEffort: null      },
  quick:      { model: "gpt",    effort: "none",   fallback: ["glm"],    fallbackEffort: null      },
};

function classifyCategory(task) {
  for (const [cat, pattern] of Object.entries(CATEGORY_PATTERNS)) {
    if (pattern.test(task)) return cat;
  }
  return null; // ë¶„ë¥˜ ë¶ˆê°€ â†’ í˜¸ì¶œìê°€ ê¸°ë³¸ê°’ ê²°ì •
}

async function callSmartRoute(task, category = null, context = null, maxTokens = null) {
  const cat = category ?? classifyCategory(task) ?? "deep";
  const routing = CATEGORY_ROUTING[cat] ?? CATEGORY_ROUTING.deep;
  const fullPrompt = context ? `[ì»¨í…ìŠ¤íŠ¸]\n${context}\n\n[ì‘ì—…]\n${task}` : task;
  const primaryModel = routing.model;

  async function tryModel(model, effort, logExtra) {
    switch (model) {
      case "gpt":
        return callGpt(fullPrompt, "gpt-5.3-codex", null, effort ?? "medium", maxTokens, logExtra);
      case "gemini":
        return callGemini(fullPrompt, "gemini-2.5-pro", null, maxTokens, null, logExtra);
      case "glm":
        return callGlm(fullPrompt, "glm-4.7-flash", null, maxTokens, null, logExtra);
      default:
        throw new Error(`ì•Œ ìˆ˜ ì—†ëŠ” ëª¨ë¸: ${model}`);
    }
  }

  // Primary ì‹œë„
  try {
    const result = await tryModel(primaryModel, routing.effort, {
      category: cat,
      routing: `smart_routeâ†’${primaryModel}`,
    });
    saveRoutingTrace({
      tool: "smart_route",
      category: cat,
      model: primaryModel,
      effort: routing.effort,
      reason: CATEGORY_REASON[cat] ?? cat,
      didFallback: false,
    });
    return result;
  } catch (primaryErr) {
    // Fallback ì²´ì¸
    for (const fbModel of routing.fallback) {
      try {
        const result = await tryModel(fbModel, routing.fallbackEffort, {
          category: cat,
          routing: `smart_routeâ†’${primaryModel}(fail)â†’${fbModel}`,
        });
        saveRoutingTrace({
          tool: "smart_route",
          category: cat,
          model: fbModel,
          effort: routing.fallbackEffort,
          reason: CATEGORY_REASON[cat] ?? cat,
          didFallback: true,
          fallbackFrom: primaryModel,
        });
        return result;
      } catch {
        // ë‹¤ìŒ í´ë°±ìœ¼ë¡œ ê³„ì†
      }
    }
    throw new Error(`smart_route ì „ì²´ ì‹¤íŒ¨ â€” ëª¨ë“  í´ë°± ì†Œì§„ (1ì°¨ ì˜¤ë¥˜: ${primaryErr.message})`);
  }
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// ask_parallel â€” Promise.allSettled() ë‹¤ì¤‘ ëª¨ë¸ ë™ì‹œ í˜¸ì¶œ
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async function callAskParallel(prompt, models = null, systemPrompt = null) {
  const ALL_MODELS = ["gpt", "gemini", "glm"];
  const selectedModels = (models ?? ALL_MODELS).filter((m) => ALL_MODELS.includes(m));
  if (selectedModels.length === 0) selectedModels.push(...ALL_MODELS);

  const logExtra = { routing: "ask_parallel" };

  const modelCalls = {
    gpt:    () => callGpt(prompt, "gpt-5.3-codex", systemPrompt, "medium", null, logExtra),
    gemini: () => callGemini(prompt, "gemini-2.5-pro", systemPrompt, null, null, logExtra),
    glm:    () => callGlm(prompt, "glm-4.7-flash", systemPrompt, null, null, logExtra),
  };

  const results = await Promise.allSettled(selectedModels.map((m) => modelCalls[m]()));

  return results
    .map((r, i) => {
      const label = selectedModels[i].toUpperCase();
      return r.status === "fulfilled"
        ? `=== ${label} [OK] ===\n${r.value}`
        : `=== ${label} [FAIL] ===\n${r.reason?.message ?? "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"}`;
    })
    .join("\n\n");
}

// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
// MCP ì„œë²„ ì •ì˜
// â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const server = new Server(
  { name: "multi-model-agent", version: "4.0.0" },
  { capabilities: { tools: {} } }
);

server.setRequestHandler(ListToolsRequestSchema, async () => ({
  tools: [
    // â”€â”€â”€ smart_route â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
      name: "smart_route",
      description: [
        "ì‘ì—…ì„ ì¹´í…Œê³ ë¦¬ë¡œ ë¶„ë¥˜í•˜ì—¬ ìµœì  ëª¨ë¸ì— ìë™ ë¼ìš°íŒ…í•©ë‹ˆë‹¤. (OMO ìŠ¤íƒ€ì¼)",
        "",
        "ã€ì¹´í…Œê³ ë¦¬ â†’ ëª¨ë¸ ë§¤í•‘ã€‘",
        "- ultrabrain : GPT(xhigh) â€” ì „ì²´ ì•„í‚¤í…ì²˜ ì„¤ê³„, ì¢…í•© ì „ëµ",
        "- deep       : GPT(high)  â€” ì•Œê³ ë¦¬ì¦˜, ë³µì¡í•œ ë””ë²„ê¹…, ë¦¬íŒ©í† ë§",
        "- visual     : Gemini     â€” UI/UX, React/Vue, í”„ë¡ íŠ¸ì—”ë“œ",
        "- research   : Gemini     â€” ì½”ë“œë² ì´ìŠ¤ ì „ì²´ ë¶„ì„, ëŒ€ê·œëª¨ íŒŒì¼",
        "- bulk       : GLM        â€” ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸, CRUD, ë°˜ë³µ íŒ¨í„´",
        "- writing    : GLM        â€” ë¬¸ì„œ, README, ì£¼ì„ ì¶”ê°€",
        "- quick      : GPT(none)  â€” ë‹¨ìˆœ ë³€í™˜, í¬ë§·íŒ…",
        "",
        "ã€í´ë°± ì²´ì¸ã€‘ primary ì‹¤íŒ¨ ì‹œ ìë™ í´ë°±",
        "- ultrabrain/deep â†’ GPT â†’ Gemini",
        "- visual/research â†’ Gemini â†’ GPT",
        "- bulk/writing    â†’ GLM â†’ Gemini",
        "- quick           â†’ GPT â†’ GLM",
      ].join("\n"),
      inputSchema: {
        type: "object",
        properties: {
          task: { type: "string", description: "ìˆ˜í–‰í•  ì‘ì—… ë‚´ìš© (í•„ìˆ˜)" },
          category: {
            type: "string",
            description: "ì¹´í…Œê³ ë¦¬ ëª…ì‹œ (ì„ íƒ, ìƒëµ ì‹œ í‚¤ì›Œë“œë¡œ ìë™ ë¶„ë¥˜)",
            enum: ["ultrabrain", "deep", "visual", "research", "bulk", "writing", "quick"],
          },
          context: { type: "string", description: "ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ (ì½”ë“œ, ë°°ê²½ ë“±)" },
          max_tokens: { type: "number", description: "ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜ (ì„ íƒ)" },
        },
        required: ["task"],
      },
    },

    // â”€â”€â”€ ask_parallel â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
      name: "ask_parallel",
      description: [
        "ê°™ì€ í”„ë¡¬í”„íŠ¸ë¥¼ ì—¬ëŸ¬ ëª¨ë¸ì— ë™ì‹œ ì „ì†¡í•˜ì—¬ ì‘ë‹µì„ ë¹„êµí•©ë‹ˆë‹¤.",
        "ì½”ë“œ ë¦¬ë·°, ì•„í‚¤í…ì²˜ êµì°¨ ê²€ì¦, ì¤‘ìš”í•œ ê¸°ìˆ  ê²°ì •ì— ìœ ìš©í•©ë‹ˆë‹¤.",
        "",
        "ì¶œë ¥: === GPT [OK] === / === GEMINI [FAIL] === í˜•ì‹ìœ¼ë¡œ ê° ëª¨ë¸ ì‘ë‹µ êµ¬ë¶„",
      ].join("\n"),
      inputSchema: {
        type: "object",
        properties: {
          prompt: { type: "string", description: "ëª¨ë“  ëª¨ë¸ì—ê²Œ ì „ë‹¬í•  í”„ë¡¬í”„íŠ¸ (í•„ìˆ˜)" },
          models: {
            type: "array",
            items: { type: "string", enum: ["gpt", "gemini", "glm"] },
            description: "ì‚¬ìš©í•  ëª¨ë¸ ëª©ë¡ (ì„ íƒ, ê¸°ë³¸: ì „ì²´ [gpt, gemini, glm])",
          },
          system_prompt: { type: "string", description: "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒ)" },
        },
        required: ["prompt"],
      },
    },

    // â”€â”€â”€ ask_gpt â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
      name: "ask_gpt",
      description: [
        "GPT-5.3-Codexì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤ (ChatGPT Plus OAuth ì¸ì¦).",
        "ë‚´ë¶€ì ìœ¼ë¡œ OpenAI Responses API(/v1/responses)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.",
        "ì‚¬ì „ ì¡°ê±´: `codex login` ì™„ë£Œ í›„ ~/.codex/auth.json ì¡´ì¬ í•„ìš”.",
        "",
        "ã€ì´ íˆ´ì´ ì í•©í•œ ì‘ì—…ã€‘",
        "- ë³µì¡í•œ ì½”ë“œ ë¦¬ë·° ë° ê°œì„  ì œì•ˆ",
        "- ì°½ì˜ì  ê¸€ì“°ê¸°, ì˜ì–´ ê¸°ìˆ  ë¬¸ì„œ ì‘ì„±",
        "- ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ ì„¤ê³„ ë° ì•„í‚¤í…ì²˜ ë¸Œë ˆì¸ìŠ¤í† ë°",
        "- ë©€í‹°ìŠ¤í… ì¶”ë¡ ì´ í•„ìš”í•œ ë””ë²„ê¹…",
        "",
        "ã€reasoning_effort ê°€ì´ë“œã€‘",
        "- none  : reasoning í† í° ì—†ìŒ (í¬ë§·íŒ…, ë‹¨ìˆœ ë³€í™˜ â€” ìµœì € ë¹„ìš©)",
        "- low   : ë¹ ë¥¸ ì‘ë‹µ (ê°„ë‹¨í•œ ì§ˆë¬¸)",
        "- medium: ê¸°ë³¸ê°’, ëŒ€ë¶€ë¶„ì˜ ì‘ì—…",
        "- high  : ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜, ê¹Šì€ ì½”ë“œ ë¶„ì„",
        "- xhigh : ì¥ì‹œê°„ ì•„í‚¤í…ì²˜ ì„¤ê³„, ì „ì²´ ì½”ë“œë² ì´ìŠ¤ ë¦¬íŒ©í† ë§",
      ].join("\n"),
      inputSchema: {
        type: "object",
        properties: {
          prompt: { type: "string", description: "GPTì—ê²Œ ì „ë‹¬í•  ì‘ì—… ë‚´ìš©" },
          model: {
            type: "string",
            description: "ì‚¬ìš©í•  GPT ëª¨ë¸",
            enum: ["gpt-5.3-codex", "gpt-5.2-codex", "gpt-5.1-codex-max"],
            default: "gpt-5.3-codex",
          },
          reasoning_effort: {
            type: "string",
            description: "ì¶”ë¡  ê°•ë„ (ê¸°ë³¸ê°’: medium)",
            enum: ["none", "low", "medium", "high", "xhigh"],
            default: "medium",
          },
          system_prompt: { type: "string", description: "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)" },
          max_tokens: { type: "number", description: "ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜ (max_output_tokensë¡œ ë§¤í•‘)" },
        },
        required: ["prompt"],
      },
    },

    // â”€â”€â”€ ask_gemini â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
      name: "ask_gemini",
      description: [
        "Gemini 2.5 Proì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤ (AI Studio API Key).",
        "",
        "ã€í•µì‹¬ ê°•ì  - ì´ ìƒí™©ì—ì„œ ë°˜ë“œì‹œ ì‚¬ìš©í•  ê²ƒã€‘",
        "- 200ì¤„ ì´ìƒ íŒŒì¼/ì½”ë“œë² ì´ìŠ¤ ì „ì²´ ë¶„ì„ (1M í† í° ì»¨í…ìŠ¤íŠ¸)",
        "- ì—¬ëŸ¬ íŒŒì¼ì„ í•œ ë²ˆì— ë„˜ê²¨ ì „ì²´ êµ¬ì¡° íŒŒì•…",
        "- React/Vue/Tailwind UI ì»´í¬ë„ŒíŠ¸ ë° í™”ë©´ ì„¤ê³„",
        "- ê¸´ ë¬¸ì„œ ìš”ì•½ ë° ë¹„êµ ë¶„ì„",
        "",
        "ã€Sonnet ëŒ€ì‹  ì´ íˆ´ì„ ì“¸ ê²ƒã€‘",
        "íŒŒì¼ì´ ê¸¸ê±°ë‚˜, ì—¬ëŸ¬ íŒŒì¼ì„ ë™ì‹œì— ë¶„ì„í•´ì•¼ í•  ë•Œ.",
      ].join("\n"),
      inputSchema: {
        type: "object",
        properties: {
          prompt: { type: "string", description: "Geminiì—ê²Œ ì „ë‹¬í•  ì‘ì—… ë‚´ìš©" },
          model: {
            type: "string",
            description: "ì‚¬ìš©í•  Gemini ëª¨ë¸",
            default: "gemini-2.5-pro",
          },
          system_prompt: { type: "string", description: "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)" },
          max_tokens: { type: "number", description: "ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 8192)" },
          temperature: { type: "number", description: "ì˜¨ë„ íŒŒë¼ë¯¸í„° 0.0~2.0 (ì„ íƒ)" },
        },
        required: ["prompt"],
      },
    },

    // â”€â”€â”€ ask_glm â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
      name: "ask_glm",
      description: [
        "GLM-4.7-Flash(Z.ai)ì—ê²Œ ì‘ì—…ì„ ìœ„ì„í•©ë‹ˆë‹¤. ë¹„ìš© íš¨ìœ¨ ìµœìš°ì„ .",
        "",
        "ã€ë°˜ë“œì‹œ ì´ íˆ´ì„ ë¨¼ì € ê³ ë ¤í•  ìƒí™©ã€‘",
        "- ë³´ì¼ëŸ¬í”Œë ˆì´íŠ¸ ì½”ë“œ (CRUD, ë§ˆì´ê·¸ë ˆì´ì…˜, ì‹œë“œ íŒŒì¼)",
        "- ë°˜ë³µ íŒ¨í„´ íŒŒì¼ ìƒì„± (ë¼ìš°í„°, ì»¨íŠ¸ë¡¤ëŸ¬, ëª¨ë¸ 2ê°œ ì´ìƒ)",
        "- í¬ë§· ë³€í™˜ (JSONâ†”CSV, SQL ìŠ¤í‚¤ë§ˆ ë³€í™˜)",
        "- ì£¼ì„ ì¶”ê°€, ì½”ë“œ ì •ë¦¬, ë‹¨ìˆœ ë¦¬íŒ©í† ë§",
        "- ë¹ ë¥¸ ì´ˆì•ˆ ì‘ì„± (ì´í›„ Sonnetì´ ë‹¤ë“¬ëŠ” ìš©ë„)",
        "",
        "ã€ì„±ëŠ¥ã€‘ SWE-bench Verified 77.8% (ì˜¤í”ˆì†ŒìŠ¤ 1ìœ„)",
        "ã€ë¹„ìš©ã€‘ Sonnet 4.6 ëŒ€ë¹„ ì•½ 5~8ë°° ì €ë ´",
      ].join("\n"),
      inputSchema: {
        type: "object",
        properties: {
          prompt: { type: "string", description: "GLM-4.7-Flashì—ê²Œ ì „ë‹¬í•  ì‘ì—… ë‚´ìš©" },
          model: {
            type: "string",
            description: "ì‚¬ìš©í•  GLM ëª¨ë¸",
            default: "glm-4.7-flash",
          },
          system_prompt: { type: "string", description: "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ (ì„ íƒì‚¬í•­)" },
          max_tokens: { type: "number", description: "ìµœëŒ€ ì¶œë ¥ í† í° ìˆ˜ (ê¸°ë³¸ê°’: 4096)" },
          temperature: { type: "number", description: "ì˜¨ë„ íŒŒë¼ë¯¸í„° 0.0~2.0 (ì„ íƒ)" },
        },
        required: ["prompt"],
      },
    },

    // â”€â”€â”€ get_usage_stats â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    {
      name: "get_usage_stats",
      description: [
        "ì™¸ë¶€ ëª¨ë¸(GLM/Gemini/GPT) í† í° ì‚¬ìš©ëŸ‰ í†µê³„ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.",
        "ëª¨ë¸ë³„ í˜¸ì¶œ ìˆ˜, ì…ë ¥/ì¶œë ¥ í† í°, ë¹„ìœ¨, ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜ í˜„í™©ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.",
      ].join("\n"),
      inputSchema: {
        type: "object",
        properties: {
          days: {
            type: "number",
            description: "ì¡°íšŒí•  ê¸°ê°„ (ì¼). ê¸°ë³¸ê°’: 7",
            default: 7,
          },
        },
      },
    },
  ],
}));

server.setRequestHandler(CallToolRequestSchema, async (request) => {
  const { name, arguments: args } = request.params;
  let result;

  try {
    switch (name) {
      case "smart_route":
        result = await callSmartRoute(
          args.task,
          args.category ?? null,
          args.context ?? null,
          args.max_tokens ?? null
        );
        break;

      case "ask_parallel":
        result = await callAskParallel(
          args.prompt,
          args.models ?? null,
          args.system_prompt ?? null
        );
        saveRoutingTrace({
          tool: "ask_parallel",
          model: "parallel",
          models: args.models ?? ["gpt", "gemini", "glm"],
        });
        break;

      case "ask_gpt":
        result = await callGpt(
          args.prompt,
          args.model ?? "gpt-5.3-codex",
          args.system_prompt ?? null,
          args.reasoning_effort ?? "medium",
          args.max_tokens ?? null
        );
        saveRoutingTrace({ tool: "ask_gpt", model: "gpt", effort: args.reasoning_effort ?? "medium" });
        break;

      case "ask_gemini":
        result = await callGemini(
          args.prompt,
          args.model ?? "gemini-2.5-pro",
          args.system_prompt ?? null,
          args.max_tokens ?? null,
          args.temperature ?? null
        );
        saveRoutingTrace({ tool: "ask_gemini", model: "gemini" });
        break;

      case "ask_glm":
        result = await callGlm(
          args.prompt,
          args.model ?? "glm-4.7-flash",
          args.system_prompt ?? null,
          args.max_tokens ?? null,
          args.temperature ?? null
        );
        saveRoutingTrace({ tool: "ask_glm", model: "glm" });
        break;

      case "get_usage_stats":
        result = getUsageStats(args?.days ?? 7);
        break;

      default:
        result = `ì•Œ ìˆ˜ ì—†ëŠ” íˆ´: ${name}`;
    }
  } catch (err) {
    result = `[ì˜¤ë¥˜] ${err.message}`;
  }

  return { content: [{ type: "text", text: result }] };
});

const transport = new StdioServerTransport();
await server.connect(transport);
