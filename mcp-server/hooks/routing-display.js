#!/usr/bin/env node
/**
 * Routing Display Hook â€” PostToolUse
 * v5.4: last-call.json ê¸°ë°˜ ë¼ìš°íŒ… ì •ë³´ ë°•ìŠ¤ ì¶œë ¥
 *
 * MCP multi-model-agent ë„êµ¬ í˜¸ì¶œ í›„ ë¼ìš°íŒ… ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
 * Claude Codeì˜ hook íŒŒì´í”„ëŠ” ANSIë¥¼ ì§€ì›í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ìœ ë‹ˆì½”ë“œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
 * ë“±ë¡: settings.json PostToolUse â€” matcher: "mcp__multi-model-agent"
 */

import { readFileSync, existsSync } from "fs";
import { homedir } from "os";
import { join } from "path";

const LAST_CALL_PATH = join(homedir(), "mcp-servers", "multi-model", "last-call.json");

const MODEL_DISPLAY = {
  "gpt-5.3-codex": "GPT-5.3-Codex",
  "glm-5":         "GLM-5",
  "parallel":      "All Models",
};

const MODEL_ICON = {
  "gpt-5.3-codex": "ğŸ§ ",
  "glm-5":         "âš¡",
  "parallel":      "ğŸ”€",
};

const CATEGORY_ICON = {
  ultrabrain: "ğŸ›",
  deep:       "ğŸ”¬",
  visual:     "ğŸ¨",
  research:   "ğŸ“š",
  bulk:       "âš™ï¸",
  writing:    "âœï¸",
  quick:      "âš¡",
};

// â”€â”€â”€ stdin ì½ê¸° (for await íŒ¨í„´ â€” Windows í˜¸í™˜) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const chunks = [];
for await (const chunk of process.stdin) chunks.push(chunk);

const raw = Buffer.concat(chunks).toString("utf8").trim();
if (!raw) process.exit(0);

let input;
try { input = JSON.parse(raw); } catch { process.exit(0); }

const toolName = input?.tool_name ?? input?.tool ?? input?.tool_use?.name ?? "";
if (!toolName.includes("multi-model-agent")) process.exit(0);

const tool = toolName.split("__").pop();
if (tool === "get_usage_stats") process.exit(0);

// â”€â”€â”€ last-call.json ì½ê¸° (index.jsê°€ ì‘ì„±) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if (!existsSync(LAST_CALL_PATH)) process.exit(0);

let meta;
try { meta = JSON.parse(readFileSync(LAST_CALL_PATH, "utf8")); } catch { process.exit(0); }

// 10ì´ˆ ì´ë‚´ ê¸°ë¡ë§Œ ì‹ ë¢°
if (meta.timestamp && Date.now() - new Date(meta.timestamp).getTime() > 10000) process.exit(0);

// â”€â”€â”€ ì¶œë ¥ êµ¬ì„± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
const modelKey  = meta.model ?? "unknown";
const modelIcon = MODEL_ICON[modelKey] ?? "ğŸ¤–";
const modelName = MODEL_DISPLAY[modelKey] ?? modelKey;
const effortStr = meta.reasoning_effort && meta.reasoning_effort !== "none"
  ? `  Â·  reasoning: ${meta.reasoning_effort}` : "";
const catIcon = meta.category ? (CATEGORY_ICON[meta.category] ?? "ğŸ“Œ") : "";

const SEP = "â”€".repeat(44);
const lines = [`â”Œâ”€ ğŸ”€ ROUTING ${SEP}`];

if (tool === "ask_parallel" || modelKey === "parallel") {
  lines.push(`â”‚  ğŸ”€  ëª¨ë“  ëª¨ë¸ ë™ì‹œ í˜¸ì¶œ (Parallel)`);
  const modelList = (meta.models ?? ["gpt", "glm"])
    .map((m) => {
      const key = m === "gpt" ? "gpt-5.3-codex" : m === "glm" ? "glm-5" : m;
      return `${MODEL_ICON[key] ?? "ğŸ¤–"} ${MODEL_DISPLAY[key] ?? m}`;
    })
    .join("  +  ");
  lines.push(`â”‚  ${modelList}`);
} else {
  lines.push(`â”‚  ${modelIcon} ${modelName}${effortStr}`);
  if (meta.category) {
    lines.push(`â”‚  ${catIcon} ${meta.category}`);
  }
  if (meta.routing && meta.routing.includes("fail")) {
    lines.push(`â”‚  âš   í´ë°± ë°œìƒ`);
  }
}

lines.push(`â””${"â”€".repeat(57)}`);

process.stdout.write(lines.join("\n") + "\n");
